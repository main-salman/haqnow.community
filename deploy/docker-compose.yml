version: "3.9"
services:
  api:
    build:
      context: ..
      dockerfile: backend/Dockerfile.deploy
    image: haqnow/api:latest
    ports:
      - "8000:8000"
    env_file:
      - ../.env
    environment:
      - PYTHONUNBUFFERED=1
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2:1b}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-mxbai-embed-large}
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request,sys; sys.exit(0) if urllib.request.urlopen('http://localhost:8000/health').getcode()==200 else sys.exit(1)\""]
      interval: 10s
      timeout: 5s
      retries: 12
    depends_on:
      redis:
        condition: service_started
      ollama:
        condition: service_started

  worker:
    build:
      context: ..
      dockerfile: backend/Dockerfile.deploy
    image: haqnow/worker:latest
    command: ["celery", "-A", "app.celery_app", "worker", "--loglevel=info"]
    env_file:
      - ../.env
    environment:
      - PYTHONUNBUFFERED=1
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2:1b}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-mxbai-embed-large}
    depends_on:
      redis:
        condition: service_started
      ollama:
        condition: service_started

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  frontend:
    build:
      context: ..
      dockerfile: frontend/Dockerfile.deploy
    image: haqnow/frontend:latest
    ports:
      - "3000:80"
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O - http://localhost/ > /dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
    depends_on:
      api:
        condition: service_healthy

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  ollama_data:
