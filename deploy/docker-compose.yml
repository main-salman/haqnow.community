version: "3.9"
services:
  api:
    image: python:3.11-slim
    working_dir: /srv
    command: bash -lc "apt-get update && apt-get install -y tesseract-ocr tesseract-ocr-eng libtesseract-dev && pip install poetry && cd backend && poetry install && poetry run uvicorn app.main:app --host 0.0.0.0 --port 8000"
    ports:
      - "8000:8000"
    volumes:
      - ..:/srv:cached
    environment:
      - PYTHONUNBUFFERED=1
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2:1b}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-mxbai-embed-large}
    depends_on:
      - redis
      - ollama

  worker:
    image: python:3.11-slim
    working_dir: /srv
    command: bash -lc "apt-get update && apt-get install -y tesseract-ocr tesseract-ocr-eng libtesseract-dev && pip install poetry && cd backend && poetry install && poetry run celery -A app.celery_app worker --loglevel=info"
    volumes:
      - ..:/srv:cached
    environment:
      - PYTHONUNBUFFERED=1
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2:1b}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-mxbai-embed-large}
    depends_on:
      - redis
      - ollama

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  frontend:
    image: node:18-alpine
    working_dir: /srv/frontend
    command: sh -c "npm install && npx vite build && npx vite preview --host 0.0.0.0 --port 3000"
    ports:
      - "3000:3000"
    volumes:
      - ../frontend:/srv/frontend:cached
    environment:
      - NODE_ENV=production
    depends_on:
      - api

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  ollama_data:
